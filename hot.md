## 热词分析 ##

1. 总体结构

  原模拟计划中的整体的数据流大致是这个样子：
    
   > 网页 === （爬虫） ===> 文本文件 === （logstash） ===> kafka === （logstash） ===> ES

  现在要在其中加入一个热词计算的部分。这部分可以放在爬虫过来的文本文件之后，也可以放在kafka之后。取决于爬虫的具体形态。也可以去掉kafka。变成：

   > 网页 === （爬虫） ===> 文本文件 === （热词处理） ===> 文本文件 === （logstash） ===> ES

   或者：

   > 网页 === （爬虫） ===> 文本文件 === （logstash） ===> kafka ===（logstash）===> 文本文件 ==== (热词处理) ===> 文本文件 === （logstash） ===> ES

   前面部分老徐的爬虫能搞定，这次只讲文本文件到热词处理开始的部分

2. 热词计算的接口要求
	
  在目前的设计中，标题和正文的权重是不同的，所以需要爬虫在输出时能区分标题和正文。目前假定爬虫的输入是以json形式存在：

  > {"title" : "tttttttt", "content" : "cccccccccccccccc"}

  一篇文章为一条json记录。可以考虑再附加其他一些属性，比如时间、来源之类。但是 title 和 content 必须要有。

  一次输入多篇文章时，需要以列表形式存在：

  > [
  >   {"title" : "t1", "content" : "c1"},
  >   {"title" : "t2", "content" : "c2"}
  > ]

3. 热词的计算方法

  热词算法以 TF-IDF 算法为基础，并根据文章长度作了权重调整，最终计算公式如下：

  >           TFIDF = Wf * ( -1 * log( TWf / TWc )) 
  >	  其中：	   
  >
  >        Wf = 当前计算的范围内的该词的出现次数
  >
  >		  TWf = 整个IDF库内该词出现的总次数
  >
  >       TWc = 整个IDF库内总词数
  >  
  >   如果词出现在标题中，则一个当两个算，即 Wf * 2
  >
  >   对数的底数为2.

  在这个算法下，计算一篇文章内部的热词，与计算一段时期内多篇文章的热词，没有本质区别。可以一次直接计算多篇文章的热词，也可以将单篇文章内的热词简单累加。这最后的结果，可以成为统计范围内的热度。而取热度排名前N的词，则为统计范围内的热词。

  统计范围可以是一篇文章，可以是一段时期内的所有文章，也可以按别的来分类。这使得可以实现如三日热词，七日热词，年度热词之类的效果，或者杭州公安热词，珠海公安热词等等。

4. 该热词算法所需的资源
  
  1. 一个分词用的词典。

    目前使用了一个来自网络的含300万词左右的词典，没有对公安系统作定制

  2. 一个分词算法。

    目前Demo中使用了结巴分词

  3. 一个IDF库。

    目前Demo中使用了697篇政府公文建立初步的IDF库。我们算法中的 IDF 是和通行的 TFIDF 不同的，不能通用。在真实系统中，建议让爬虫先扒一定量的数据打底，以建立自己的IDF库。之后在处理新数据时，程序也会对IDF库作更新。

  4. 一个停止词表。

    目前使用的是一个含746个中英文停止词以及数字标点的停止词表。

5. 热词处理的输出

  输出形式可以有两种。一种是输出一份包含当前计算的除停止词外的所有词及其TF-IDF值（热度）的列表。一种是输出一份当前计算的热度排名前N的词的列表。
  如果是输出到ES的话，我倾向于输出全部词的TF-IDF值。前N个词的提取可以由ES处理。
  

6. 应用展望

  以公安系统网页新闻更新的频繁程度来看，可以以月为单位作一次热词统计。将该月内的数据全部输出到热词处理程序，计算后，记录下日期和各词在这个时间段内的热度。用时间做index，输出到ES中。前端程序可以使用这个数据生成每月热词报表，以及某词在各个月中的热度变化。还可以将各月热度相加，获得多个月或者一年的热度。

7. 通过logstash输出到ES

  因为是每月计算一次，所以可以通过timestamp来划定统计范围，故而无需额外设置type进行区分。全部内容都放在一个type里。

  logstash的配置：

  '''
  	input {
  		file{  
  			path => "/root/NLPKEYWord/output/tfidf.txt"   #必须写绝对路径
  			start_position => "beginning"
  		}
  	}

  	filter{
  		grok{
  			match => {"message" => "%{DATA:word}\t%{NUMBER:freq:float}\r"}   #测试环境中的tfidf.txt文件是在Windows下生成的，换行符多个\r。Linux环境下，末尾的\r不需要加上
  			remove_field => {"message"}
  		}
  	}

  	output{
  		# stdout { codec => "rubydebug" }
  		elasticsearch{
  			hosts => ["127.0.0.1:39200"]
  			index => "hotword"
  		}
  	}
  '''
